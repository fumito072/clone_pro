"""
ã‚·ãƒ³ãƒ—ãƒ«ãªRAGå®Ÿè£… - JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹
å¤–éƒ¨DBãªã—ã€JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çŸ¥è­˜ã‚’èª­ã¿è¾¼ã‚“ã§æ¤œç´¢
"""
import json
import re
from pathlib import Path
from typing import List, Dict, Tuple


class SimpleRAG:
    """JSONå½¢å¼ã®ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ã£ãŸã‚·ãƒ³ãƒ—ãƒ«ãªRAG"""
    
    def __init__(self, knowledge_dir: Path):
        """
        RAGã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
        
        Args:
            knowledge_dir: ãƒŠãƒ¬ãƒƒã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ*.jsonï¼‰ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.knowledge_dir = Path(knowledge_dir)
        self.chunks: List[Dict] = []
        self._load_knowledge()
    
    def _load_knowledge(self):
        """JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒŠãƒ¬ãƒƒã‚¸ã‚’èª­ã¿è¾¼ã¿"""
        if not self.knowledge_dir.exists():
            print(f"âš ï¸  [RAG] ãƒŠãƒ¬ãƒƒã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.knowledge_dir}")
            return
        
        json_files = list(self.knowledge_dir.glob("*.json"))
        if not json_files:
            print(f"âš ï¸  [RAG] JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.knowledge_dir}")
            return
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    # JSONLå½¢å¼ã‚’æƒ³å®šï¼ˆ1è¡Œ1JSONï¼‰
                    for line_num, line in enumerate(f, 1):
                        line = line.strip()
                        if line:
                            try:
                                chunk = json.loads(line)
                                self.chunks.append(chunk)
                            except json.JSONDecodeError as e:
                                print(f"âš ï¸  [RAG] {json_file.name}:{line_num} JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
            except Exception as e:
                print(f"âš ï¸  [RAG] {json_file.name}ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        print(f"âœ… [RAG] ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿å®Œäº†: {len(self.chunks)}ä»¶")
    
    def _tokenize(self, text: str) -> set:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        
        Args:
            text: ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚»ãƒƒãƒˆ
        """
        # ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ãƒ»è‹±æ•°å­—ã‚’æŠ½å‡º
        tokens = re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF\w]+', text.lower())
        return set(tokens)
    
    def search(self, query: str, top_k: int = 3, min_score: float = 0.0) -> List[Dict]:
        """
        ã‚·ãƒ³ãƒ—ãƒ«ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã§é–¢é€£ãƒãƒ£ãƒ³ã‚¯ã‚’æ¤œç´¢
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            top_k: è¿”ã™æœ€å¤§ä»¶æ•°
            min_score: æœ€å°ã‚¹ã‚³ã‚¢ï¼ˆã“ã‚Œä»¥ä¸‹ã¯é™¤å¤–ï¼‰
        
        Returns:
            é–¢é€£ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        """
        if not self.chunks:
            return []
        
        # ã‚¯ã‚¨ãƒªã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        query_tokens = self._tokenize(query)
        
        if not query_tokens:
            return []
        
        # å„ãƒãƒ£ãƒ³ã‚¯ã¨ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        scored_chunks: List[Tuple[float, Dict]] = []
        
        for chunk in self.chunks:
            text = chunk.get('text', '')
            text_tokens = self._tokenize(text)
            
            if not text_tokens:
                continue
            
            # Jaccardé¡ä¼¼åº¦ï¼ˆé›†åˆã®é¡ä¼¼åº¦ï¼‰
            intersection = query_tokens & text_tokens
            union = query_tokens | text_tokens
            score = len(intersection) / len(union) if union else 0
            
            if score > min_score:
                scored_chunks.append((score, chunk))
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
        scored_chunks.sort(reverse=True, key=lambda x: x[0])
        
        # Top-Kã‚’è¿”ã™
        top_results = scored_chunks[:top_k]
        
        if top_results:
            print(f"ğŸ’¡ [RAG] æ¤œç´¢ãƒ’ãƒƒãƒˆ: {len(top_results)}ä»¶ï¼ˆã‚¹ã‚³ã‚¢: {top_results[0][0]:.3f}ã€œ{top_results[-1][0]:.3f}ï¼‰")
        
        return [chunk for score, chunk in top_results]
    
    def format_context(self, chunks: List[Dict]) -> str:
        """
        æ¤œç´¢çµæœã‚’æ–‡å­—åˆ—ã«æ•´å½¢
        
        Args:
            chunks: æ¤œç´¢çµæœã®ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        
        Returns:
            æ•´å½¢ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—
        """
        if not chunks:
            return ""
        
        context_parts = []
        for i, chunk in enumerate(chunks, 1):
            text = chunk.get('text', '')
            date = chunk.get('date', '')
            chunk_id = chunk.get('chunk_id', '')
            speaker = chunk.get('speaker', '')
            
            # ç™ºè¨€è€…ã¨æ—¥ä»˜æƒ…å ±ã‚’å«ã‚ã‚‹
            metadata = []
            if speaker:
                metadata.append(f"ç™ºè¨€è€…: {speaker}")
            if date:
                metadata.append(f"æ—¥ä»˜: {date}")
            if chunk_id:
                metadata.append(f"ID: {chunk_id}")
            
            metadata_str = ", ".join(metadata) if metadata else "æƒ…å ±ãªã—"
            
            context_parts.append(
                f"ã€å‚è€ƒæƒ…å ± {i}ã€‘({metadata_str})\n{text}"
            )
        
        return "\n\n".join(context_parts)
    
    def get_stats(self) -> Dict:
        """
        ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
        
        Returns:
            çµ±è¨ˆæƒ…å ±ã®è¾æ›¸
        """
        if not self.chunks:
            return {
                "total_chunks": 0,
                "speakers": [],
                "dates": []
            }
        
        speakers = set()
        dates = set()
        
        for chunk in self.chunks:
            if 'speaker' in chunk:
                speakers.add(chunk['speaker'])
            if 'date' in chunk:
                dates.add(chunk['date'])
        
        return {
            "total_chunks": len(self.chunks),
            "speakers": sorted(list(speakers)),
            "dates": sorted(list(dates))
        }
