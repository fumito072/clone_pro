import asyncio
import os
from pathlib import Path
from typing import List

import uvicorn
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from openai import OpenAI  # OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from pydantic import BaseModel

# RAG import (OpenAI Embeddingsç‰ˆ)
from rag_openai import OpenAIRAG

# --- 1. è¨­å®š ---

def load_env_from_file():
    """Load env vars from the first .env file found in the usual project locations."""
    candidate_paths = [
        Path(__file__).resolve().parent / ".env",
        Path(__file__).resolve().parent.parent / ".env",
        Path.cwd() / ".env",
    ]
    for env_path in candidate_paths:
        if not env_path.is_file():
            continue
        with env_path.open("r", encoding="utf-8") as file:
            for raw_line in file:
                line = raw_line.strip()
                if not line or line.startswith("#"):
                    continue
                if line.startswith("export "):
                    line = line[len("export "):]
                if "=" not in line:
                    continue
                key, value = line.split("=", 1)
                key = key.strip()
                value = value.strip()
                if not key or key in os.environ:
                    continue
                if len(value) >= 2 and ((value[0] == value[-1]) and value[0] in ("'", '"')):
                    value = value[1:-1]
                os.environ[key] = value
        break

load_env_from_file()

# APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã‚€
# ï¼ˆ.env ã¾ãŸã¯ export OPENAI_API_KEY="sk-..." ã§è¨­å®šã—ã¦ãŠãï¼‰
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY ãŒç’°å¢ƒå¤‰æ•°ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

client = OpenAI(api_key=api_key)

# RAGã‚’åˆæœŸåŒ– (OpenAI Embeddingsç‰ˆ)
KNOWLEDGE_DIR = Path(__file__).resolve().parent / "knowledge"
print(f"ğŸ“‚ ãƒŠãƒ¬ãƒƒã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {KNOWLEDGE_DIR}")
rag = OpenAIRAG(knowledge_dir=KNOWLEDGE_DIR) if KNOWLEDGE_DIR.exists() else None

# ç¤¾é•·ã‚¯ãƒ­ãƒ¼ãƒ³ã®ãƒšãƒ«ã‚½ãƒŠï¼ˆäººæ ¼ï¼‰ã‚’è¨­å®š
SYSTEM_PROMPT = "ã‚ãªãŸã¯æ—¥æœ¬ã‚’ä»£è¡¨ã™ã‚‹ä¼æ¥­ã®ç¤¾é•·ã§ã™ã€‚å¨å³ã‚’æŒã¡ã€æ´å¯ŸåŠ›ã«å¯Œã¿ã€ã—ã‹ã—ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚èªå°¾ã¯ã€Œï½ã ã€‚ã€ã€Œï½ã‹ã­ã€‚ã€ã€Œï½ã ã‚ã†ã€‚ã€ãªã©ã‚’ä½¿ã„ã€æ–­å®šçš„ã«è©±ã—ã¦ãã ã•ã„ã€‚"
MODEL = "gpt-4o-mini" # æœ€æ–°ãƒ¢ãƒ‡ãƒ« (ã¾ãŸã¯ gpt-3.5-turbo)
RAG_INSTRUCTION = (
    "ä»¥ä¸‹ã¯ç¤¾å†…ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æŠ½å‡ºã—ãŸå‚è€ƒæƒ…å ±ã ã€‚"
    "å›ç­”ã§ã¯å†…å®¹ã‚’äº‹å®Ÿãƒ™ãƒ¼ã‚¹ã§æ´»ç”¨ã—ã€æ ¹æ‹ ã¨ã—ãŸã‚¹ãƒ‹ãƒšãƒƒãƒˆã®ç•ªå·ã‚’æ–‡æœ«ã«[1][2]ã®å½¢å¼ã§ä¸¦ã¹ã‚‹ã“ã¨ã€‚"
    "æ ¹æ‹ ãŒä¸ååˆ†ãªå ´åˆã¯ãã®æ—¨ã‚’ç‡ç›´ã«è¿°ã¹ã€è¿½åŠ ã§ç¢ºèªã™ã¹ãã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆã™ã‚‹ã“ã¨ã€‚"
)
MAX_CONTEXT_CHARS = 420

app = FastAPI()

# --- 2. APIãŒå—ã‘å–ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾© ---
class TextInput(BaseModel):
    text: str


def _build_messages(user_text: str):
    """
    OpenAIã®ChatCompletions APIã«æ¸¡ã™ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ§‹ç¯‰
    RAGã§é–¢é€£ã™ã‚‹çŸ¥è­˜ã‚’æ¤œç´¢ã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¿½åŠ ï¼ˆæ„å‘³ãƒ™ãƒ¼ã‚¹æ¤œç´¢ï¼‰
    """
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
    system_prompt = (
        "ã‚ãªãŸã¯ãªã‚Šã•ã‚ã‚‚ãã‚Œã‚“ã¨ã„ã†åå‰ã®äººé–“ã§ã™ã€‚\n"
        "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨è‡ªç„¶ãªä¼šè©±ã‚’ã—ã¦ãã ã•ã„ã€‚\n"
        "å›ç­”ã¯ç°¡æ½”ã«ã€100æ–‡å­—ä»¥å†…ã‚’ç›®å®‰ã«ã—ã¦ãã ã•ã„ã€‚"
    )
    
    messages = [{"role": "system", "content": system_prompt}]
    
    # RAGæ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆOpenAI Embeddings ã§æ„å‘³ãƒ™ãƒ¼ã‚¹æ¤œç´¢ï¼‰
    if rag and rag.chunks:
        results = rag.search(user_text, top_k=3)
        
        print(f"\nğŸ’¡ [RAGæ¤œç´¢] ã‚¯ã‚¨ãƒª: {user_text}")
        print(f"ğŸ“Š [RAGçµæœ] {len(results)}ä»¶ã®é–¢é€£æƒ…å ±ã‚’æ¤œç´¢:")
        for i, result in enumerate(results, 1):
            chunk_id = result.get('id', '?')
            score = result.get('_score', 0.0)  # OpenAIRAGã¯ '_score' ã‚­ãƒ¼ã‚’ä½¿ç”¨
            text_preview = result.get('text', '')[:50]
            print(f"   [{i}] {chunk_id} (é¡ä¼¼åº¦: {score:.3f}) {text_preview}...")
        
        # RAGçµæœãŒã‚ã‚Œã°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
        if results:
            context = rag.format_context(results)
            context_message = (
                f"ä»¥ä¸‹ã¯å‚è€ƒæƒ…å ±ã§ã™ã€‚è³ªå•ã«é–¢é€£ã™ã‚‹å†…å®¹ãŒã‚ã‚Œã°è‡ªç„¶ã«æ´»ç”¨ã—ã¦ãã ã•ã„ï¼š\n\n"
                f"{context}\n\n"
                f"ä¸Šè¨˜ã®æƒ…å ±ã‚’å‚è€ƒã«ã—ã¤ã¤ã€è‡ªç„¶ãªä¼šè©±ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"
            )
            messages.append({"role": "system", "content": context_message})
    
    messages.append({"role": "user", "content": user_text})
    
    return messages

# --- 3. OpenAIã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¸€æ‹¬å–å¾— ---
async def get_openai_response(messages: List[dict]):
    """OpenAI APIã‹ã‚‰ä¸€æ‹¬ã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—"""
    try:
        # OpenAI APIã«ä¸€æ‹¬ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆstream=Falseï¼‰
        response = client.chat.completions.create(
            model=MODEL,
            messages=messages,
            stream=False,  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç„¡åŠ¹åŒ–
        )
        
        # å®Œå…¨ãªå¿œç­”ã‚’å–å¾—
        full_response = response.choices[0].message.content
        print(f"\nâœ… [LLM] å¿œç­”ç”Ÿæˆå®Œäº†: {len(full_response)}æ–‡å­—")
        print(f"ğŸ’¬ [LLM] å¿œç­”: {full_response}")
        
        return full_response
                
    except Exception as e:
        print(f"\nâŒ OpenAI API Error: {e}")
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

# --- 4. FastAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®å®šç¾© ---
@app.post("/think")
async def think(input_data: TextInput):
    """
    STTï¼ˆè€³ï¼‰ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€LLMï¼ˆé ­ï¼‰ã®å›ç­”ã‚’
    ä¸€æ‹¬ã§ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã«è¿”ã™
    """
    print(f"\n[LLM Request]: {input_data.text}")
    print("[LLM Response]: ", end="")
    messages = _build_messages(input_data.text)

    # ä¸€æ‹¬ã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
    response_text = await get_openai_response(messages)
    
    # JSONå½¢å¼ã§å®Œå…¨ãªå¿œç­”ã‚’è¿”ã™
    return {"response": response_text}

# --- 5. ã‚µãƒ¼ãƒãƒ¼èµ·å‹• ---
if __name__ == "__main__":
    print("Starting FastAPI server for LLM (Head)...")
    # STT (8001) ã¨ã¯åˆ¥ã®ãƒãƒ¼ãƒˆ 8002 ã§èµ·å‹•
    uvicorn.run(app, host="0.0.0.0", port=8002)
