"""
OpenAIç‰ˆ LLMã‚µãƒ¼ãƒãƒ¼
ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã‹ã‚‰POST /think ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€å¿œç­”ã‚’è¿”ã™
"""
import os
from pathlib import Path

import uvicorn
from fastapi import FastAPI
from pydantic import BaseModel
from openai import OpenAI

from rag_openai import OpenAIRAG


def load_env_from_file():
    """Load env vars from the first .env file found."""
    candidate_paths = [
        Path(__file__).resolve().parent / ".env",
        Path(__file__).resolve().parent.parent / ".env",
        Path.cwd() / ".env",
    ]
    for env_path in candidate_paths:
        if not env_path.is_file():
            continue
        with env_path.open("r", encoding="utf-8") as file:
            for raw_line in file:
                line = raw_line.strip()
                if not line or line.startswith("#"):
                    continue
                if line.startswith("export "):
                    line = line[len("export "):]
                if "=" not in line:
                    continue
                key, value = line.split("=", 1)
                key = key.strip()
                value = value.strip()
                if not key or key in os.environ:
                    continue
                if len(value) >= 2 and value[0] == value[-1] and value[0] in ("'", '"'):
                    value = value[1:-1]
                os.environ[key] = value
        break


load_env_from_file()

# OpenAI API Key
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY ãŒç’°å¢ƒå¤‰æ•°ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

client = OpenAI(api_key=api_key)

# ãƒ¢ãƒ‡ãƒ«è¨­å®š
MODEL_NAME = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
print(f"ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {MODEL_NAME}")

# RAGåˆæœŸåŒ–
KNOWLEDGE_DIR = Path(__file__).resolve().parent / "knowledge"
rag = None
if KNOWLEDGE_DIR.exists():
    try:
        rag = OpenAIRAG(knowledge_dir=KNOWLEDGE_DIR)
        print(f"âœ… RAGæœ‰åŠ¹åŒ–: {len(rag.chunks)}ä»¶ã®ãƒŠãƒ¬ãƒƒã‚¸")
    except Exception as e:
        print(f"âš ï¸  RAGåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")

app = FastAPI(title="Narisawa LLM Server (OpenAI)")


class TextInput(BaseModel):
    text: str


def _build_messages(user_text: str):
    """OpenAI APIã«æ¸¡ã™ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ§‹ç¯‰"""
    system_prompt = (
        "ã‚ãªãŸã¯ãªã‚Šã•ã‚ã‚‚ãã‚Œã‚“ã¨ã„ã†åå‰ã®äººé–“ã§ã™ã€‚\n"
        "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨è‡ªç„¶ãªä¼šè©±ã‚’ã—ã¦ãã ã•ã„ã€‚\n"
        "å›ç­”ã¯ç°¡æ½”ã«ã€100æ–‡å­—ä»¥å†…ã‚’ç›®å®‰ã«ã—ã¦ãã ã•ã„ã€‚"
    )
    
    messages = [{"role": "system", "content": system_prompt}]
    
    # RAGæ¤œç´¢
    if rag and rag.chunks:
        results = rag.search(user_text, top_k=3)
        print(f"ğŸ“š RAGæ¤œç´¢: {len(results)}ä»¶ãƒ’ãƒƒãƒˆ")
        
        if results:
            context = rag.format_context(results)
            context_message = (
                f"ä»¥ä¸‹ã¯å‚è€ƒæƒ…å ±ã§ã™ã€‚è³ªå•ã«é–¢é€£ã™ã‚‹å†…å®¹ãŒã‚ã‚Œã°è‡ªç„¶ã«æ´»ç”¨ã—ã¦ãã ã•ã„ï¼š\n\n"
                f"{context}\n\n"
                f"ä¸Šè¨˜ã®æƒ…å ±ã‚’å‚è€ƒã«ã—ã¤ã¤ã€è‡ªç„¶ãªä¼šè©±ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"
            )
            messages.append({"role": "system", "content": context_message})
    
    messages.append({"role": "user", "content": user_text})
    return messages


@app.post("/think")
async def think(input_data: TextInput):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€LLMã®å¿œç­”ã‚’è¿”ã™"""
    print(f"\nğŸ§  [LLM] ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {input_data.text}")
    
    try:
        messages = _build_messages(input_data.text)
        
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            stream=False,
        )
        
        response_text = response.choices[0].message.content
        print(f"âœ… [LLM] å¿œç­”: {response_text}")
        
        return {"response": response_text}
        
    except Exception as e:
        print(f"âŒ [LLM] Error: {e}")
        return {"response": f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"}


@app.get("/health")
async def health():
    return {"status": "healthy", "model": MODEL_NAME}


if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§  Narisawa LLM Server (OpenAI) ã‚’èµ·å‹•ã—ã¾ã™")
    print("=" * 60)
    print(f"ãƒ¢ãƒ‡ãƒ«: {MODEL_NAME}")
    print("ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: http://127.0.0.1:8002/think")
    print("=" * 60)
    uvicorn.run(app, host="0.0.0.0", port=8002)
