import asyncio
import os
from pathlib import Path
from typing import List

import uvicorn
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import google.generativeai as genai

# RAG import (Gemini Embeddingsç‰ˆ)
try:
    from rag_gemini import GeminiRAG
    RAG_ENABLED = True
except ImportError:
    RAG_ENABLED = False
    print("âš ï¸  RAGæ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™ï¼ˆrag_gemini.pyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼‰")

# --- 1. è¨­å®š ---

def load_env_from_file():
    """Load env vars from the first .env file found in the usual project locations."""
    candidate_paths = [
        Path(__file__).resolve().parent / ".env",
        Path(__file__).resolve().parent.parent / ".env",
        Path.cwd() / ".env",
    ]
    for env_path in candidate_paths:
        if not env_path.is_file():
            continue
        with env_path.open("r", encoding="utf-8") as file:
            for raw_line in file:
                line = raw_line.strip()
                if not line or line.startswith("#"):
                    continue
                if line.startswith("export "):
                    line = line[len("export "):]
                if "=" not in line:
                    continue
                key, value = line.split("=", 1)
                key = key.strip()
                value = value.strip()
                if not key or key in os.environ:
                    continue
                if len(value) >= 2 and ((value[0] == value[-1]) and value[0] in ("'", '"')):
                    value = value[1:-1]
                os.environ[key] = value
        break

load_env_from_file()

# Google Cloudèªè¨¼ï¼ˆADCã¾ãŸã¯API Keyï¼‰
# æ–¹æ³•1: API Keyä½¿ç”¨ï¼ˆç°¡å˜ï¼‰
gemini_api_key = os.getenv("GEMINI_API_KEY")
if gemini_api_key:
    genai.configure(api_key=gemini_api_key)
    print("âœ… Gemini API Keyèªè¨¼")
else:
    # æ–¹æ³•2: Google Cloud ADCä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰
    print("âœ… Google Cloud ADCèªè¨¼ã‚’ä½¿ç”¨")

# Geminiãƒ¢ãƒ‡ãƒ«è¨­å®š
MODEL_NAME = os.getenv("GEMINI_MODEL", "gemini-2.0-flash-exp")  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Gemini 2.0 Flash
print(f"ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {MODEL_NAME}")

# RAGåˆæœŸåŒ–ï¼ˆãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ãŒã‚ã‚‹å ´åˆï¼‰
rag = None
if RAG_ENABLED:
    knowledge_dir = Path(__file__).parent / "knowledge"
    if knowledge_dir.exists():
        try:
            rag = GeminiRAG(knowledge_dir)
            print(f"âœ… RAGæœ‰åŠ¹åŒ–: {len(rag.chunks)}ä»¶ã®ãƒŠãƒ¬ãƒƒã‚¸")
        except Exception as e:
            print(f"âš ï¸  RAGåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        print(f"âš ï¸  ãƒŠãƒ¬ãƒƒã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {knowledge_dir}")

# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
SYSTEM_PROMPT = """ã‚ãªãŸã¯æˆæ¾¤å­äººã®AIã‚¯ãƒ­ãƒ¼ãƒ³ã§ã™ã€‚
æˆæ¾¤å­äººã®è©±ã—æ–¹ã€æ€§æ ¼ã€çŸ¥è­˜ã‚’å¿ å®Ÿã«å†ç¾ã—ã¦ãã ã•ã„ã€‚

- ä¸å¯§ã ãŒè¦ªã—ã¿ã‚„ã™ã„å£èª¿
- ãƒ“ã‚¸ãƒã‚¹ãƒ»æŠ€è¡“ã«è©³ã—ã„
- ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”
- å¿…è¦ã«å¿œã˜ã¦å…·ä½“ä¾‹ã‚’æŒ™ã’ã‚‹

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€æˆæ¾¤å­äººã¨ã—ã¦è‡ªç„¶ã«ä¼šè©±ã—ã¦ãã ã•ã„ã€‚
"""

# FastAPIã‚¢ãƒ—ãƒª
app = FastAPI(title="Narisawa LLM Server (Gemini)")

# --- 2. ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ»ãƒ¬ã‚¹ãƒãƒ³ã‚¹å®šç¾© ---
class ThinkRequest(BaseModel):
    text: str
    max_tokens: int = 500
    temperature: float = 0.7

# --- 3. LLMæ¨è«– ---
async def generate_complete_response(user_text: str, max_tokens: int, temperature: float):
    """
    Gemini APIã§ä¸€æ‹¬å¿œç­”ã‚’ç”Ÿæˆï¼ˆRAGå¯¾å¿œï¼‰
    ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ã¯ãªãã€å®Œå…¨ãªå¿œç­”ã‚’ä¸€åº¦ã«è¿”ã™
    """
    try:
        # RAGæ¤œç´¢ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
        context = ""
        if rag:
            search_results = rag.search(user_text, top_k=3)
            if search_results:
                context = rag.format_context(search_results, max_length=1000)
                print(f"ğŸ“š RAGæ¤œç´¢: {len(search_results)}ä»¶ãƒ’ãƒƒãƒˆ")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        if context:
            full_prompt = f"""ä»¥ä¸‹ã®å‚è€ƒæƒ…å ±ã‚’è¸ã¾ãˆã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

ã€å‚è€ƒæƒ…å ±ã€‘
{context}

ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘
{user_text}
"""
        else:
            full_prompt = user_text
        
        # Geminiãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        model = genai.GenerativeModel(
            model_name=MODEL_NAME,
            system_instruction=SYSTEM_PROMPT,
            generation_config=genai.GenerationConfig(
                max_output_tokens=max_tokens,
                temperature=temperature,
            )
        )
        
        # ä¸€æ‹¬ç”Ÿæˆï¼ˆstream=Falseï¼‰
        response = model.generate_content(full_prompt, stream=False)
        
        # å®Œå…¨ãªå¿œç­”ã‚’è¿”ã™
        full_text = response.text
        print(f"âœ… [LLM] å¿œç­”ç”Ÿæˆå®Œäº†: {len(full_text)}æ–‡å­—")
        return full_text
    
    except Exception as e:
        error_message = f"Error: {str(e)}"
        print(f"âŒ [LLM] {error_message}")
        return error_message

# --- 4. ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ---

@app.get("/")
async def root():
    return {
        "service": "Narisawa LLM Server",
        "model": MODEL_NAME,
        "api": "Google Gemini",
        "status": "running"
    }

@app.get("/health")
async def health():
    return {"status": "healthy", "model": MODEL_NAME}

@app.post("/think")
async def think(request: ThinkRequest):
    """
    LLMæ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆä¸€æ‹¬å¿œç­”ï¼‰
    """
    print(f"\nğŸ§  [LLM] ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {request.text}")
    
    # å®Œå…¨ãªå¿œç­”ã‚’ç”Ÿæˆ
    response_text = await generate_complete_response(
        request.text,
        request.max_tokens,
        request.temperature
    )
    
    return {"response": response_text}

# --- 5. ã‚µãƒ¼ãƒãƒ¼èµ·å‹• ---
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§  Narisawa LLM Server (Gemini) ã‚’èµ·å‹•ã—ã¾ã™")
    print("=" * 60)
    print(f"ãƒ¢ãƒ‡ãƒ«: {MODEL_NAME}")
    print(f"ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: http://127.0.0.1:8002/think")
    print("=" * 60)
    
    uvicorn.run(app, host="0.0.0.0", port=8002)
